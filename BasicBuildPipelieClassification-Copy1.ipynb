{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535c32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging\n",
    "from azureml.core.model import Model\n",
    "from azureml.exceptions import WebserviceException\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e75d2",
   "metadata": {},
   "source": [
    "## Setting up Key Vault Values - Do not keep these around - run 1 time only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1466b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import azureml.core\n",
    "# import os, shutil\n",
    "# from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "# from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "# os.environ.setdefault('tenantId', 'XXXX')\n",
    "# os.environ.setdefault('servicePrincipalId', 'XXXX')\n",
    "# os.environ.setdefault('servicePrincipalPassword', 'XXXXXX')\n",
    "# os.environ.setdefault('wsName', 'XXX')\n",
    "# os.environ.setdefault('subscriptionId', 'XXXXX')\n",
    "# os.environ.setdefault('resourceGroup', 'XXXXX')\n",
    "\n",
    "\n",
    "# environment_variables = ['tenantId', 'servicePrincipalId', 'servicePrincipalPassword', 'wsName', \n",
    "#                          'subscriptionId', 'resourceGroup']\n",
    "\n",
    "# envs = {}\n",
    "\n",
    "# for x in environment_variables:\n",
    "#     print(x, \"=\", os.environ.get(x))\n",
    "#     envs[x] = os.environ.get(x)\n",
    "\n",
    "\n",
    "# sp = ServicePrincipalAuthentication(tenant_id=envs['tenantId'], # tenantID\n",
    "#                                     service_principal_id=envs['servicePrincipalId'], # clientId\n",
    "#                                     service_principal_password=envs['servicePrincipalPassword']) # clientSecret\n",
    "\n",
    "# ws = Workspace.get(name=envs['wsName'],\n",
    "#                    auth=sp,\n",
    "#                    subscription_id=envs['subscriptionId'],\n",
    "#                    resource_group=envs['resourceGroup'])\n",
    "# ws.get_details()\n",
    "# keyvault = ws.get_default_keyvault()\n",
    "\n",
    "# for x in environment_variables:\n",
    "#     print(x, \"=\", os.environ.get(x))\n",
    "#     keyvault.set_secret(name = x, value = envs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb88252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_environment_variables():\n",
    "#     global envs\n",
    "#     global run_by_notebook \n",
    "#     run_by_notebook = False\n",
    "#     environment_variables = ['tenantId', 'servicePrincipalId', 'servicePrincipalPassword', 'wsName', \n",
    "#                          'subscriptionId', 'resourceGroup']\n",
    "#     envs = {}\n",
    "#     for x in environment_variables:\n",
    "#         if os.environ.get(x) == None:\n",
    "#             #get the values from keyvault\n",
    "#             run_by_notebook = True\n",
    "#             print('retrieve from key vault, value is None: ' + x)\n",
    "#             ws = Workspace.from_config()\n",
    "#             keyvault = ws.get_default_keyvault()\n",
    "#             kv_results = keyvault.get_secrets(environment_variables)\n",
    "#             envs = kv_results\n",
    "#             for x in envs:\n",
    "#                 os.environ.setdefault(x, envs[x])\n",
    "#             exit\n",
    "#         else:\n",
    "#             envs[x] = os.environ.get(x)\n",
    "#     return run_by_notebook\n",
    "\n",
    "\n",
    "\n",
    "# get_environment_variables()\n",
    "# sp = ServicePrincipalAuthentication(tenant_id=envs['tenantId'], # tenantID\n",
    "#                                     service_principal_id=envs['servicePrincipalId'], # clientId\n",
    "#                                     service_principal_password=envs['servicePrincipalPassword']) # clientSecret\n",
    "# ws = Workspace.get(name=envs['wsName'],\n",
    "#                        auth=sp,\n",
    "#                        subscription_id=envs['subscriptionId'],\n",
    "#                        resource_group=envs['resourceGroup'])\n",
    "# ws.get_details()\n",
    "\n",
    "# print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n",
    "# print('Run By Notebook:' + str(run_by_notebook))\n",
    "# # Get the default datastore\n",
    "# default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bff7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "default_ds = ws.get_default_datastore()\n",
    "run_by_notebook = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b578401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = 'aml-cluster001'\n",
    "model_name_to_register = 'text_classification_001'\n",
    "experiment_folder = 'text_classificatin_example'\n",
    "conda_yml_file = 'textclassification_env.yml'\n",
    "registered_env_name = \"text-classificiation-env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c502e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Select AML Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26731050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inital_model_version = 4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    initial_model = Model(ws, model_name_to_register)\n",
    "    inital_model_version = initial_model.version\n",
    "except WebserviceException :\n",
    "    inital_model_version = 0\n",
    "print('inital_model_version = ' + str(inital_model_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd71ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_classificatin_example\n",
      "continue run_outputs directory does not exits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "# Create a folder for the pipeline step files\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(experiment_folder)\n",
    "\n",
    "run_path = './run_outputs'\n",
    "try:\n",
    "    shutil.rmtree(run_path)\n",
    "except:\n",
    "    print('continue run_outputs directory does not exits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e07df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"assetId\": \"azureml://locations/eastus2/workspaces/9634b616-29cb-4345-ae22-4be4e4bfe009/environments/text-classificiation-env/versions/3\",\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20221101.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"buildContext\": null,\n",
       "        \"enabled\": false,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"text-classificiation-env\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"dependencies\": [\n",
       "                \"python=3.8.5\",\n",
       "                \"scikit-learn\",\n",
       "                \"ipykernel\",\n",
       "                \"matplotlib\",\n",
       "                \"pandas\",\n",
       "                \"pip\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\",\n",
       "                        \"numpy\",\n",
       "                        \"joblib\",\n",
       "                        \"sklearn\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"name\": \"textclassification_env\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"3\"\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment.from_conda_specification(registered_env_name, conda_yml_file)\n",
    "env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ed7db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    registered_env = Environment.get(ws, registered_env_name)\n",
    "    pipeline_run_config = RunConfiguration()\n",
    "    \n",
    "    # Use the compute you created above. \n",
    "    pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "    # Assign the environment to the run configuration\n",
    "    pipeline_run_config.environment = registered_env\n",
    "    print (\"Run configuration created.\")\n",
    "except Exception as e: \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811f05d",
   "metadata": {},
   "source": [
    "## Define Output datasets\n",
    "\n",
    "The OutputFileDatasetConfig object is a special kind of data reference that is used for interim storage locations that can be passed between pipeline steps, so you'll create one and use at as the output for the first step and the input for the second step. Note that you need to pass it as a script argument so your code can access the datastore location referenced by the data reference.\n",
    "\n",
    "Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the register_on_complete() call.\n",
    "\n",
    "These can be viewed in the Datasets tab directly in the AML Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_to_register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1006ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data  = OutputFileDatasetConfig(name='training_data', destination=(default_ds, model_name_to_register + '_training_data/{run-id}')).read_delimited_files().register_on_complete(name= model_name_to_register + '_training_data')\n",
    "testing_data   = OutputFileDatasetConfig(name='testing_data',  destination=(default_ds, model_name_to_register +  '_testing_data/{run-id}')).read_delimited_files().register_on_complete(name= model_name_to_register + '_testing_data')\n",
    "\n",
    "model_file     = PipelineData(name='model_file', datastore=default_ds)\n",
    "\n",
    "model_name         = PipelineParameter(\"model_name\", default_value= model_name_to_register)\n",
    "model_desc         = PipelineParameter(\"model_desc\", default_value=model_name_to_register + ' description')\n",
    "raw_file_location  = PipelineParameter(name=\"raw_file_location\", default_value='spam-data/spamformodel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2c4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26038b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/classifier_training.py\n",
    "\n",
    "import argparse\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def summarize_classification(y_test, y_pred, run):\n",
    "    acc = accuracy_score(y_test, y_pred, normalize=True) #how many predictions correct %\n",
    "    num_acc = accuracy_score(y_test, y_pred, normalize = False)\n",
    "    prec = precision_score(y_test, y_pred, average = 'weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    \n",
    "    run.log('total count', len(y_test))\n",
    "    run.log('acc count', num_acc)\n",
    "    run.log('Accuracy', acc)\n",
    "    run.log('prec', prec)\n",
    "    run.log('recall', recall)\n",
    "    \n",
    "    run.parent.log('acc count', num_acc)\n",
    "    run.parent.log('Accuracy', acc)\n",
    "    run.parent.log('prec', prec)\n",
    "    run.parent.log('recall', recall)\n",
    "\n",
    "\n",
    "    print('accuracy count:', num_acc)\n",
    "    print('accuracy score:', acc)\n",
    "    print('precision:', prec)\n",
    "    print('recall:', recall)\n",
    "    \n",
    "\n",
    "\n",
    "def getRuntimeArgs():\n",
    "    # Get script arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--raw_file_location', dest='raw_file_location', required=True)\n",
    "    parser.add_argument('--training_data', dest='training_data', required=True)\n",
    "    parser.add_argument('--testing_data', dest='testing_data', required=True)\n",
    "    parser.add_argument('--model_file', dest='model_file', required=True)\n",
    "    parser.add_argument('--model_name', dest='model_name', required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "def model_train(ds_df, run, training_data, testing_data):\n",
    "    \n",
    "    clf = Pipeline([\n",
    "                            ('count_vectorizer', CountVectorizer()),\n",
    "                            ('classifier', LogisticRegression(solver='lbfgs', max_iter=10000))\n",
    "                        ])\n",
    "    #output of convectorizer, feed to classifier\n",
    "    train, test = train_test_split(ds_df, test_size=0.2, random_state=0)\n",
    "\n",
    "    x_train = train['text']\n",
    "    y_train = train['labels']\n",
    "    \n",
    "    x_test = test['text']\n",
    "    y_test = test['labels']\n",
    "    \n",
    "    \n",
    "    print(train.head())\n",
    "\n",
    "\n",
    "    model = clf.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    print('*************************')\n",
    "    print('model predictions:')\n",
    "    print(y_pred)\n",
    "    summarize_classification(y_test, y_pred, run)\n",
    "    \n",
    "    os.makedirs(training_data, exist_ok=True)\n",
    "    os.makedirs(testing_data, exist_ok=True)\n",
    "\n",
    "    test['results'] = y_pred\n",
    "    print('training_data = ' + training_data)\n",
    "    print('testing_data = ' + testing_data)\n",
    "    train.to_csv(os.path.join(training_data, 'training_data.csv'), index=False )\n",
    "    test.to_csv(os.path.join(testing_data, 'testing_data.csv'), index=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = getRuntimeArgs()\n",
    "    \n",
    "    raw_file_location = args.raw_file_location\n",
    "    training_data = args.training_data\n",
    "    testing_data = args.testing_data\n",
    "    model_file = args.model_file\n",
    "    model_name = args.model_name\n",
    "    \n",
    "    print(\"training_data =\" + training_data)\n",
    "    print('testing_data =' + testing_data)\n",
    "    # Get the experiment run context\n",
    "    run = Run.get_context()\n",
    "    ws = run.experiment.workspace\n",
    "    print(ws)\n",
    "    ds = ws.get_default_datastore()\n",
    "    \n",
    "    \n",
    "    print(\"Loading Data...\")\n",
    "    dataset = Dataset.Tabular.from_delimited_files(path = [(ds, raw_file_location)])\n",
    "    data = dataset.to_pandas_dataframe()\n",
    "    \n",
    "    \n",
    "    print(data.columns)\n",
    "    lr = model_train(data, run, training_data,  testing_data)\n",
    "    \n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    model_file_name = model_name  + '.pkl'\n",
    "    file_name = './outputs/' +model_file_name\n",
    "    \n",
    "    print(\"Joblib Version : \", joblib.__version__)\n",
    "    \n",
    "    joblib.dump(value=lr, filename=file_name)\n",
    "\n",
    "    #copy to pass model to next step as the model file\n",
    "    os.makedirs(model_file, exist_ok=True)\n",
    "    shutil.copyfile(file_name, os.path.join(model_file, model_file_name))\n",
    "\n",
    "    run.complete()\n",
    "\n",
    " \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad4f74",
   "metadata": {},
   "source": [
    "## Train Model Python Script Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_step = PythonScriptStep(\n",
    "    name='Get Data and Create Model',\n",
    "    script_name='classifier_training.py',\n",
    "    arguments =['--raw_file_location', raw_file_location,\n",
    "                '--training_data', training_data,\n",
    "                '--testing_data', testing_data,\n",
    "                '--model_file', model_file,\n",
    "                '--model_name', model_name\n",
    "               ],\n",
    "    inputs=[],\n",
    "    outputs=[model_file, training_data, testing_data],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28088b79",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd27090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/eval.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.core.model import Model\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"Evaluate model and register if more performant\")\n",
    "\n",
    "parser.add_argument('--model_name', dest='model_name', required=True)\n",
    "parser.add_argument('--model_file', dest = 'model_file',  required=True)\n",
    "parser.add_argument('--model_desc', dest = 'model_desc',  required=True)\n",
    "parser.add_argument('--deploy_file', dest='deploy_file', required=True)\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "model_name = args.model_name\n",
    "model_file = args.model_file\n",
    "model_desc = args.model_desc\n",
    "\n",
    "deploy_file = args.deploy_file\n",
    "\n",
    "#Get current run\n",
    "run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "#Get default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "#Get metrics associated with current parent run\n",
    "metrics = run.get_metrics()\n",
    "\n",
    "print('current run metrics')\n",
    "for key in metrics.keys():\n",
    "        print(key, metrics.get(key))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('parent run metrics')\n",
    "#Get metrics associated with current parent run\n",
    "metrics = run.parent.get_metrics()\n",
    "\n",
    "for key in metrics.keys():\n",
    "        print(key, metrics.get(key))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "current_model_acc_count = float(metrics['acc count'])\n",
    "current_model_acc = float(metrics['Accuracy'])\n",
    "current_model_prec = float(metrics['prec'])\n",
    "current_model_recall = float(metrics['recall'])\n",
    "    \n",
    "\n",
    "# Get current model from workspace\n",
    "\n",
    "model_description = model_desc\n",
    "model_list = Model.list(ws, name=model_name, latest=True)\n",
    "first_registration = len(model_list)==0\n",
    "\n",
    "updated_tags = {'Accuracy': current_model_acc, 'prec': current_model_prec, 'recall': current_model_recall}\n",
    "\n",
    "\n",
    "print('updated tags')\n",
    "print(updated_tags)\n",
    "\n",
    "\n",
    "#upload model to the outputs directory\n",
    "relative_model_path = 'outputs'\n",
    "run.upload_folder(name=relative_model_path, path=model_file)\n",
    "\n",
    "model_file_name = model_name  + '.pkl'\n",
    "\n",
    "###################\n",
    "test_model = joblib.load(model_file + '/' + model_file_name)\n",
    "test_dataset = run.input_datasets['testing_data']\n",
    "df_test = test_dataset.to_pandas_dataframe()\n",
    "X = df_test['text']\n",
    "# Make predictions with new dataframe\n",
    "predictions = test_model.predict(X)\n",
    "print('predictions')\n",
    "print(predictions)\n",
    "#########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#If no model exists register the current model\n",
    "if first_registration:\n",
    "    print('First model registration.')\n",
    "    model_reg = run.register_model(model_path='outputs/' + model_file_name, model_name=model_name,\n",
    "                   tags=updated_tags,\n",
    "                   properties=updated_tags)\n",
    "else:\n",
    "    #If a model has been registered previously, check to see if current model \n",
    "    #performs better. If so, register it.\n",
    "    print(dir(model_list[0]))\n",
    "    if float(model_list[0].tags['prec']) < current_model_prec:\n",
    "        print('New model performs better than existing model. Register it.')\n",
    "\n",
    "        model_reg = run.register_model(model_path='outputs/' + model_file_name, model_name=model_name,\n",
    "                   tags=updated_tags,\n",
    "                   properties=updated_tags)\n",
    "        \n",
    "        # Output accuracy to file\n",
    "        with open(deploy_file, 'w+') as f:\n",
    "            f.write(('deploy_model'))\n",
    "    \n",
    "    else:\n",
    "        print('New model does not perform better than existing model. Cancel run.')\n",
    "        \n",
    "        with open(deploy_file, 'w+') as f:\n",
    "            f.write(('do not deploy model'))\n",
    "            \n",
    "        run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1bb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_file = PipelineData(name='deploy_file', datastore=default_ds)\n",
    "\n",
    "evaluate_and_register_step = PythonScriptStep(\n",
    "    name='Evaluate and Register Model',\n",
    "    script_name='eval.py',\n",
    "    arguments=[\n",
    "        '--model_name', model_name,\n",
    "        '--model_file', model_file,\n",
    "        '--model_desc', model_desc,\n",
    "        '--deploy_file', deploy_file,       \n",
    "    ],\n",
    "    inputs=[model_file.as_input('model_file'),\n",
    "            training_data.as_input(name='training_data'),\n",
    "            testing_data.as_input(name='testing_data')\n",
    "           ],\n",
    "    outputs=[ deploy_file],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9266ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/deploy.py\n",
    "\n",
    "import argparse\n",
    "from azureml.core import Workspace, Environment\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AciWebservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Deploy arg parser')\n",
    "parser.add_argument('--scoring_file_output', type=str, help='File storing the scoring url')\n",
    "parser.add_argument('--deploy_file', type=str, help='File storing if model should be deployed')\n",
    "parser.add_argument('--environment_name', type=str,help='Environment name')\n",
    "parser.add_argument('--service_name', type=str,help='service name')\n",
    "parser.add_argument('--model_name', type=str,help='model name')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "scoring_url_file = args.scoring_file_output\n",
    "deploy_file      = args.deploy_file\n",
    "environment_name = args.environment_name\n",
    "service_name     = args.service_name\n",
    "model_name       = args.model_name\n",
    "\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "model = Model(ws, model_name)\n",
    "env = Environment.get(ws, environment_name)\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "\n",
    "#test_dataset = run.input_datasets['testing_data']\n",
    "\n",
    "print(deploy_file)\n",
    "with open(deploy_file, 'r+') as f:\n",
    "    result = f.read()\n",
    "\n",
    "print(result)\n",
    "print(type(result))\n",
    "\n",
    "if result == 'do not deploy model':\n",
    "    print('do not update the deployed model')\n",
    "    run.complete()\n",
    "\n",
    "else:\n",
    "    print('continue with deploying model...')        \n",
    "    # Deploy model\n",
    "    aci_config = AciWebservice.deploy_configuration(\n",
    "                cpu_cores = 1, \n",
    "                memory_gb = 2, \n",
    "                tags = {'model': 'diabetes remote training'},\n",
    "                auth_enabled=True,\n",
    "                enable_app_insights=True)\n",
    "\n",
    "    try:\n",
    "        service = Webservice(ws, name=service_name)\n",
    "        if service:\n",
    "            service.delete()\n",
    "    except WebserviceException as e:\n",
    "             print()\n",
    "\n",
    "    service = Model.deploy(ws, service_name, [model], inference_config, aci_config, overwrite=True)\n",
    "    service.wait_for_deployment(True)\n",
    "\n",
    "\n",
    "    # Output scoring url\n",
    "    print(service.scoring_uri)\n",
    "    with open(scoring_url_file, 'w+') as f:\n",
    "        f.write(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/score.py\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#version 2\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    #Print statement for appinsights custom traces:\n",
    "    print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))\n",
    "    # Get the path to the deployed model file and load it\n",
    "    path = os.path.join(Model.get_model_path('text_classification_001'))\n",
    "    \n",
    "    print(path)\n",
    "    model = joblib.load(path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        # Get the input data as a numpy array\n",
    "        #data = np.array(json.loads(raw_data)['data'])\n",
    "        # Get a prediction from the model\n",
    "        \n",
    "        json_data = json.loads(raw_data)\n",
    "        predictions = model.predict(json_data['data'])\n",
    "        print (\"Prediction created\" + time.strftime(\"%H:%M:%S\"))\n",
    "        print(predictions)\n",
    "        # Get the corresponding classname for each prediction (0 or 1)\n",
    "        #classnames = ['not-diabetic', 'diabetic']\n",
    "        predicted_classes = []\n",
    "        for prediction in predictions:\n",
    "            #val = int(prediction)\n",
    "            predicted_classes.append(prediction)\n",
    "        # Return the predictions as JSON\n",
    "        \n",
    "         # Log the input and output data to appinsights:\n",
    "        info = {\n",
    "            \"input\": raw_data,\n",
    "            \"output\": predicted_classes\n",
    "            }\n",
    "        print(json.dumps(info))\n",
    "        \n",
    "\n",
    "        return json.dumps(predicted_classes)\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        print (error + time.strftime(\"%H:%M:%S\"))\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eabbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_file = PipelineData(name='scoring_file', datastore=default_ds)\n",
    "\n",
    "aci_service_name = 'text-classification'\n",
    "\n",
    "env_name = PipelineParameter(name='environment_name', default_value=registered_env_name)\n",
    "service_name = PipelineParameter(name='service_name', default_value=aci_service_name)\n",
    "scoring_file = PipelineData(name='scoring_file', datastore=default_ds)\n",
    "\n",
    "########################################################\n",
    "\n",
    "deploy_test = PythonScriptStep(\n",
    "    name='Deploy to ACI',\n",
    "    script_name='deploy.py',\n",
    "    arguments=[\n",
    "        '--scoring_file_output', scoring_file,\n",
    "        '--deploy_file', deploy_file,\n",
    "        '--environment_name', env_name,\n",
    "        '--service_name', service_name,\n",
    "        '--model_name', model_name\n",
    "        \n",
    "    ],\n",
    "    inputs=[\n",
    "        deploy_file.as_input('deploy_file'),\n",
    "            \n",
    "    ],\n",
    "    outputs=[scoring_file],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Pipeline Steps\n",
    "pipeline = Pipeline(workspace=ws, steps=[train_model_step, evaluate_and_register_step, deploy_test])\n",
    "if run_by_notebook:\n",
    "    experiment = Experiment(ws, 'AML_Manual_PipelineTraining')\n",
    "else:\n",
    "    experiment = Experiment(ws, 'AML_AutoDevOps_PipelineTraining')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    final_model = Model(ws, model_name_to_register)\n",
    "    final_model_version = final_model.version\n",
    "except WebserviceException :\n",
    "    final_model_version = 0\n",
    "    \n",
    "print('inital_model_version = ' + str(inital_model_version))\n",
    "print('final_model_version= ' + str(final_model_version))\n",
    "\n",
    "status = run.get_status()\n",
    "run_details = run.get_details()\n",
    "\n",
    "print((run_details))\n",
    "print(run_details['runId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e98a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_model_version > 0 and (inital_model_version != final_model_version):\n",
    "    deploy = 'deploy'\n",
    "    model_details = {\n",
    "        \"name\" : final_model.name,\n",
    "        \"version\": final_model.version,\n",
    "        \"properties\": final_model.properties,\n",
    "        \"nextstep\": \"deploy\"\n",
    "    }\n",
    "    print(model_details)\n",
    "else:\n",
    "    deploy = 'no'\n",
    "    print('do not deploy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in final_model.properties:\n",
    "    print(x)\n",
    "    print(final_model.properties[x])\n",
    "\n",
    "print (final_model.properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95409c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "outputfolder = 'run_outputs'\n",
    "os.makedirs(outputfolder, exist_ok=True)\n",
    "\n",
    "if (final_model_version != inital_model_version):\n",
    "    print('new model registered')\n",
    "    with open(os.path.join(outputfolder, 'deploy_details.json'), \"w+\") as f:\n",
    "        f.write(str(model_details))\n",
    "    model_name = model_name_to_register\n",
    "    model_list = Model.list(ws, name=model_name_to_register, latest=True)\n",
    "    model_path = model_list[0].download(exist_ok=True)\n",
    "    model_file_name = model_name_to_register + '.pkl'\n",
    "    shutil.copyfile(model_file_name,  os.path.join(outputfolder,model_file_name))\n",
    "    \n",
    "    #create model.yml file.\n",
    "    with open(os.path.join(outputfolder, 'model.yml'), \"w+\") as f:\n",
    "        f.write('$schema: https://azuremlschemas.azureedge.net/latest/model.schema.json \\n')\n",
    "        f.write('name: ' + model_details['name'] + '\\n')\n",
    "        f.write('path: ' + model_name_to_register + '.pkl \\n')\n",
    "        f.write('description: Model created from local file. \\n')\n",
    "        if len(final_model.properties) > 0:\n",
    "            f.write('properties: ')\n",
    "            f.write(json.dumps(final_model.properties))\n",
    "            f.write('\\n')\n",
    "            f.write('tags: ')\n",
    "            f.write(json.dumps(final_model.properties))\n",
    "            \n",
    "    \n",
    "with open(os.path.join(outputfolder, 'run_details.json'), \"w+\") as f:\n",
    "    print(run_details)\n",
    "    f.write(str(run_details))\n",
    "\n",
    "with open(os.path.join(outputfolder, \"run_number.json\"), \"w+\") as f:\n",
    "    f.write(run_details['runId'])\n",
    "    \n",
    "with open(os.path.join(outputfolder, \"deploy.txt\"), \"w+\") as f:\n",
    "    f.write(deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fd2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "def published_pipeline_to_pipeline_endpoint(\n",
    "    workspace,\n",
    "    published_pipeline,\n",
    "    pipeline_endpoint_name,\n",
    "    pipeline_endpoint_description=\"Endpoint to Email Classification Training pipeline\",\n",
    "):\n",
    "    try:\n",
    "        pipeline_endpoint = PipelineEndpoint.get(\n",
    "            workspace=workspace, name=pipeline_endpoint_name\n",
    "        )\n",
    "        print(\"using existing PipelineEndpoint...\")\n",
    "        pipeline_endpoint.add_default(published_pipeline)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        # create PipelineEndpoint if it doesn't exist\n",
    "        print(\"PipelineEndpoint does not exist, creating one for you...\")\n",
    "        pipeline_endpoint = PipelineEndpoint.publish(\n",
    "            workspace=workspace,\n",
    "            name=pipeline_endpoint_name,\n",
    "            pipeline=published_pipeline,\n",
    "            description=pipeline_endpoint_description\n",
    "        )\n",
    "\n",
    "if deploy == 'deploy':\n",
    "    print('deploy Email Classification Training Pipeline')\n",
    "    pipeline_endpoint_name = 'Email Classification Training Pipeline'\n",
    "    pipeline_endpoint_description = 'Endpoint to Email Classification Training pipeline'\n",
    "\n",
    "    published_pipeline = pipeline.publish(name=pipeline_endpoint_name,\n",
    "                                         description=pipeline_endpoint_description,\n",
    "                                         continue_on_step_failure=False)\n",
    "\n",
    "    published_pipeline_to_pipeline_endpoint(\n",
    "        workspace=ws,\n",
    "        published_pipeline=published_pipeline,\n",
    "        pipeline_endpoint_name=pipeline_endpoint_name,\n",
    "        pipeline_endpoint_description=pipeline_endpoint_description\n",
    "    )\n",
    "else:\n",
    "    print('do not publish pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc48073e",
   "metadata": {},
   "source": [
    "## Testing out endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b596bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "\n",
    "#get service endpoint\n",
    "service = Webservice(workspace=ws, name='text-classification')\n",
    "print(service.state)\n",
    "url = service.scoring_uri\n",
    "print(url)\n",
    "\n",
    "primary, secondary = service.get_keys()\n",
    "print(primary)\n",
    "api_key = primary\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "\n",
    "\n",
    "def MakePrediction():\n",
    "    endpoint_url = url\n",
    "    x_new = ['this is data']\n",
    "    input_json = json.dumps({\"data\": x_new})\n",
    "    print(input_json)\n",
    "    body = input_json\n",
    "    r = requests.post(endpoint_url, headers=headers, data=body)\n",
    "    return (r.json())\n",
    "\n",
    "\n",
    "results = MakePrediction()\n",
    "print(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4677cca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5c173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
